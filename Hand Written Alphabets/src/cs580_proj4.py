# -*- coding: utf-8 -*-
"""CS580-Proj4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pN3LGL0xqlCiKtGgfcELV_PBmVCX5QBn
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

colnames=['lettr', 'x-box', 'y-box', 'width','high','onpix','x-bar','y-bar','x2bar','y2bar','xybar','x2ybr','xy2br','x-ege','xegvy','y-ege','yegvx'] 
print(len(colnames))

df_Train=pd.read_csv('/content/drive/MyDrive/train.csv',sep=",",names=colnames, header=None)
df_Test=pd.read_csv('/content/drive/MyDrive/test.csv',sep=",",names=colnames, header=None)

df_Train.head()

df_Train.describe()

df_Test.describe()

#!pip install pandas-profiling==3.4.0
#from pandas_profiling import ProfileReport
#profile = ProfileReport(df)

#profile.to_file(output_file="pandas_profiling1.html")

df_Train['lettr'].value_counts()

df_Train['lettr'].value_counts().plot(kind='bar')

df_Test['lettr'].value_counts().plot(kind='bar')

df_Test['lettr'].value_counts()

df_Train.shape

df_Test.shape

df_Train.info()

df_Test.info()

df_Train.isnull().sum()

df_Test.isnull().sum()

y_Train = df_Train['lettr']
y_Train.head()

y_Test = df_Test['lettr']
y_Test.head()

df_Train.drop(['lettr'],axis=1,inplace = True)
df_Test.drop(['lettr'],axis=1,inplace = True)

print(df_Train.shape)
print(df_Test.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_Train, y_Train, test_size=0.2, random_state=0)
cv_X_train, cv_X_test, cv_y_train, cv_y_test = train_test_split(X_train, y_train, test_size=0.2)

print(cv_X_train.shape)
print(cv_X_test.shape)
print(cv_y_train.shape)
print(cv_y_test.shape)
print(X_test.shape)
print(y_test.shape)

from sklearn.preprocessing import LabelEncoder
Label_enc = LabelEncoder()

Label_enc.fit(y_Train)
y_Train = Label_enc.transform(y_Train)
y_train = Label_enc.transform(y_train)
y_test = Label_enc.transform(y_test)
cv_y_train = Label_enc.transform(cv_y_train)
cv_y_test = Label_enc.transform(cv_y_test)
y_Test = Label_enc.transform(y_Test)

print(y_Test)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(df_Train)

X_Train_mm = scaler.transform(df_Train)
X_train_mm = scaler.transform(X_train)
X_test_mm = scaler.transform(X_test)
cv_X_train_mm = scaler.transform(cv_X_train)
cv_X_test_mm = scaler.transform(cv_X_test)
X_Test_mm = scaler.transform(df_Test)

print(X_train_mm[0])

print(type(X_train_mm[0]))

from sklearn.neural_network import MLPClassifier
nnmodel = MLPClassifier(random_state=0)

nnmodel.fit(cv_X_train_mm,cv_y_train)
y_pred=nnmodel.predict(cv_X_test_mm)
print(classification_report(y_pred,cv_y_test))
cm=confusion_matrix(y_pred,cv_y_test)
print(cm)

from sklearn.preprocessing import StandardScaler

scaler1 = StandardScaler()
scaler1.fit(df_Train)

X_Train_sc = scaler1.transform(df_Train)
X_train_sc = scaler1.transform(X_train)
X_test_sc = scaler1.transform(X_test)
cv_X_train_sc = scaler1.transform(cv_X_train)
cv_X_test_sc = scaler1.transform(cv_X_test)
X_Test_sc = scaler1.transform(df_Test)

#from sklearn.model_selection import GridSearchCV

#parameters = {'hidden_layer_sizes': [(100,),(100,10),(100,20),(100,30),(250,300)],'activation': ['tanh', 'relu','logistic'],'solver': ['sgd', 'adam','lbfgs'],'alpha': 10.0 ** -np.arange(1, 5),'learning_rate': ['constant','adaptive']}
#clf = GridSearchCV(MLPClassifier(),param_grid = parameters, scoring = 'accuracy',cv = 5, n_jobs=-1)
#clf.fit(cv_X_train_mm,cv_y_train)

#print("Tuned Hyperparameters :", clf.best_params_)
#print("Accuracy :",clf.best_score_)

MPL1 = MLPClassifier(activation= 'tanh', alpha = 0.1, hidden_layer_sizes = (100,), learning_rate = 'constant', solver = 'lbfgs')

MPL1.fit(cv_X_train_mm,cv_y_train)
y_pred=nnmodel.predict(cv_X_test_mm)
print(classification_report(y_pred,cv_y_test))
cm=confusion_matrix(y_pred,cv_y_test)
print(cm)

from sklearn.neural_network import MLPClassifier
nnmodel = MLPClassifier(random_state=0)

nnmodel.fit(cv_X_train_sc,cv_y_train)
y_pred=nnmodel.predict(cv_X_test_sc)
print(classification_report(y_pred,cv_y_test))
cm=confusion_matrix(y_pred,cv_y_test)
print(cm)

predicted_class=nnmodel.predict(X_Test_sc)
print(classification_report(predicted_class,y_Test))
cm=confusion_matrix(predicted_class,y_Test)
print(cm)

#from sklearn.model_selection import GridSearchCV

#parameters = {'hidden_layer_sizes': [(100,),(100,3),(250,300)],'activation': ['tanh', 'relu','logistic'],'solver': ['sgd', 'adam','lbfgs'],'alpha': 10.0 ** -np.arange(1, 9),'learning_rate': ['constant','adaptive']}
#clf = GridSearchCV(MLPClassifier(),param_grid = parameters, scoring = 'accuracy',cv = 5, n_jobs=-1)
#clf.fit(cv_X_train_sc,cv_y_train)

#print("Tuned Hyperparameters :", clf.best_params_)
#print("Accuracy :",clf.best_score_)

from sklearn.ensemble import RandomForestClassifier
dt = RandomForestClassifier(n_estimators=100)
dt.fit(cv_X_train_sc,cv_y_train)
y_pred=dt.predict(cv_X_test_sc)
print(classification_report(y_pred,cv_y_test))
cm=confusion_matrix(y_pred,cv_y_test)
print(cm)

#from sklearn.model_selection import GridSearchCV
#parameters = [{'n_estimaters;:[100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400] ,'criterion':['gini','entropy'],'bootstrap': False}]
#grid_search = GridSearchCV(estimator =dt,param_grid = parameters,scoring = 'accuracy',cv = 5,verbose=0)
#grid_search.fit(cv_X_train_sc,cv_y_train)

#print("Tuned Hyperparameters :", grid_search.best_params_)
#print("Accuracy :",grid_search.best_score_)

predicted_class=dt.predict(X_Test_sc)
print(classification_report(predicted_class,y_Test))
cm=confusion_matrix(predicted_class,y_Test)
print(cm)

from sklearn.ensemble import RandomForestClassifier
dt = RandomForestClassifier(n_estimators=100)
dt.fit(X_Train_sc,y_Train)
y_pred=dt.predict(X_Test_sc)
print(classification_report(y_pred,y_Test))
cm=confusion_matrix(y_pred,y_Test)
print(cm)

nnmodel1 = MLPClassifier()
nnmodel1.fit(X_Train_sc,y_Train)
y_pred=nnmodel1.predict(X_Test_sc)
print(classification_report(y_pred,y_Test))
cm=confusion_matrix(y_pred,y_Test)
print(cm)

#X_Train_mm = scaler.transform(df_Train)
#X_train_mm = scaler.transform(X_train)
#X_test_mm = scaler.transform(X_test)
#cv_X_train_mm = scaler.transform(cv_X_train)
#cv_X_test_mm = scaler.transform(cv_X_test)
#X_Test_mm = scaler.transform(df_Test)

colnames1 = colnames[1:]
print(colnames1)
df_Train_mm = pd.DataFrame(X_Train_mm, columns = colnames1)
df_Test_mm = pd.DataFrame(X_Test_mm, columns = colnames1)
df_X_train_mm = pd.DataFrame(X_train_mm, columns = colnames1)
df_cv_X_train_mm = pd.DataFrame(cv_X_train_mm, columns = colnames1)
df_cv_X_test_mm = pd.DataFrame(cv_X_test_mm, columns = colnames1)
df_X_test_mm = pd.DataFrame(X_test_mm, columns = colnames1)

from sklearn.preprocessing import StandardScaler

scaler2 = StandardScaler()
scaler2.fit(df_Train_mm)

X_Train_mm_sc = scaler2.transform(df_Train_mm)
X_train_mm_sc = scaler2.transform(df_X_train_mm)
X_test_mm_sc = scaler2.transform(df_X_test_mm)
cv_X_train_mm_sc = scaler2.transform(df_cv_X_train_mm)
cv_X_test_mm_sc = scaler2.transform(df_cv_X_test_mm)
X_Test_mm_sc = scaler2.transform(df_Test_mm)

from sklearn.model_selection import ShuffleSplit,cross_val_score
from sklearn.ensemble import RandomForestClassifier
dt = RandomForestClassifier(n_estimators=100)
dt.fit(X_Train_mm_sc,y_Train)
y_pred=dt.predict(X_Test_mm_sc)
print(classification_report(y_pred,y_Test))
cm=confusion_matrix(y_pred,y_Test)
print(cm)
cv1 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
scores = cross_val_score(dt, X_Train_mm_sc, y_Train, cv=cv1)
print ('Cross validation', scores)

from sklearn.model_selection import ShuffleSplit,cross_val_score
nnmodel2 = MLPClassifier()
nnmodel2.fit(X_Train_mm_sc,y_Train)
y_pred=nnmodel2.predict(X_Test_mm_sc)
print(classification_report(y_pred,y_Test))
cm=confusion_matrix(y_pred,y_Test)
print(cm)
cv1 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
scores = cross_val_score(nnmodel2, X_Train_mm_sc, y_Train, cv=cv1)
print ('Cross validation', scores)

# from sklearn.metrics import plot_confusion_matrix
# print(y_Train)
# plot_confusion_matrix(nnmodel2, X_Test_mm_sc,y_Test)

#from sklearn.model_selection import GridSearchCV

#parameters = {'hidden_layer_sizes': [(100,),(100,10),(100,20),(100,30)],'activation': ['tanh', 'relu'],'solver': ['sgd', 'adam','lbfgs'],'alpha': 10.0 ** -np.arange(1, 10),'learning_rate': ['constant','adaptive']}
#clf = GridSearchCV(MLPClassifier(),param_grid = parameters, scoring = 'accuracy',cv = 5, n_jobs=-1)
#clf.fit(cv_X_train_mm_sc,cv_y_train)

#print("Tuned Hyperparameters :", clf.best_params_)
#print("Accuracy :",clf.best_score_)

nnmodel3 = MLPClassifier(hidden_layer_sizes=(250,300),activation='logistic')
nnmodel3.fit(X_Train_mm_sc,y_Train)
y_pred=nnmodel3.predict(X_Test_mm_sc)
print(classification_report(y_pred,y_Test))
cm=confusion_matrix(y_pred,y_Test)
print(cm)
cv1 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
scores = cross_val_score(nnmodel2, X_Train_mm_sc, y_Train, cv=cv1)
print ('Cross validation', scores)

from sklearn.model_selection import ShuffleSplit,cross_val_score
from sklearn.ensemble import RandomForestClassifier
dt = RandomForestClassifier(bootstrap= False,n_estimators=1400)
dt.fit(X_Train_mm_sc,y_Train)
y_pred=dt.predict(X_Test_mm_sc)
print(classification_report(y_pred,y_Test))
cm=confusion_matrix(y_pred,y_Test)
print(cm)
cv1 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
scores = cross_val_score(dt, X_Train_mm_sc, y_Train, cv=cv1)
print ('Cross validation', scores)

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense

model = tf.keras.models.Sequential([
  # tf.keras.layers.Normalization(axis=None),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(26,activation='sigmoid')
])
model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

model.fit(
    X_Train_mm_sc,
    y_Train,
    epochs=100
)

model.summary()

for layer in model.layers:
  weights = layer.get_weights()
  #print(weights)

from keras.utils import plot_model
plot_model(model, to_file='model.png', show_shapes=True,)

score = model.evaluate(X_Test_mm_sc, y_Test,verbose=1)
print(score)